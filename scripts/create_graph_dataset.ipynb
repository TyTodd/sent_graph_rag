{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to Converting a Dataset to a Sentence Graph Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tytodd/miniconda3/envs/sent_graph_rag/lib/python3.11/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from sent_graph_rag.Datasets import SentenceGraphCreator\n",
    "import spacy\n",
    "import pickle\n",
    "import graph_tool as gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'fastcoref' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m disable_progress_bar()\n\u001b[32m      3\u001b[39m nlp = spacy.load(\u001b[33m\"\u001b[39m\u001b[33men_core_web_sm\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfastcoref\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda:0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43menable_progress_bar\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sent_graph_rag/lib/python3.11/site-packages/spacy/language.py:824\u001b[39m, in \u001b[36mLanguage.add_pipe\u001b[39m\u001b[34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[39m\n\u001b[32m    820\u001b[39m     pipe_component, factory_name = \u001b[38;5;28mself\u001b[39m.create_pipe_from_source(\n\u001b[32m    821\u001b[39m         factory_name, source, name=name\n\u001b[32m    822\u001b[39m     )\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     pipe_component = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_pipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfactory_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    831\u001b[39m pipe_index = \u001b[38;5;28mself\u001b[39m._get_pipe_index(before, after, first, last)\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._pipe_meta[name] = \u001b[38;5;28mself\u001b[39m.get_factory_meta(factory_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sent_graph_rag/lib/python3.11/site-packages/spacy/language.py:693\u001b[39m, in \u001b[36mLanguage.create_pipe\u001b[39m\u001b[34m(self, factory_name, name, config, raw_config, validate)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_factory(factory_name):\n\u001b[32m    686\u001b[39m     err = Errors.E002.format(\n\u001b[32m    687\u001b[39m         name=factory_name,\n\u001b[32m    688\u001b[39m         opts=\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.factory_names),\n\u001b[32m   (...)\u001b[39m\u001b[32m    691\u001b[39m         lang_code=\u001b[38;5;28mself\u001b[39m.lang,\n\u001b[32m    692\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[32m    694\u001b[39m pipe_meta = \u001b[38;5;28mself\u001b[39m.get_factory_meta(factory_name)\n\u001b[32m    695\u001b[39m \u001b[38;5;66;03m# This is unideal, but the alternative would mean you always need to\u001b[39;00m\n\u001b[32m    696\u001b[39m \u001b[38;5;66;03m# specify the full config settings, which is not really viable.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: [E002] Can't find factory for 'fastcoref' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, en.lemmatizer"
     ]
    }
   ],
   "source": [
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp.add_pipe(\"fastcoref\",  config={'device': 'cuda:0', \"enable_progress_bar\": False})\n",
    "nlp.add_pipe(\"fastcoref\",  config={'device': 'cpu', \"enable_progress_bar\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MIT Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"/content/drive/MyDrive/NLP_Project/mit_wiki.txt\"\n",
    "with open(corpus_path, \"r\") as f:\n",
    "    corpus = f.read()\n",
    "    corpus2 = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grapher = SentenceGraphCreator(nlp, verbose=True)\n",
    "graphs = grapher.create_graphs([corpus], graph_type=\"gt\")\n",
    "graph, vertex_map = graphs[0]\n",
    "# graph2, vertex_map2 = graphs[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = vertex_map[\"The Massachusetts Institute of Technology_ORG_TERMINAL\"]\n",
    "v2 = vertex_map[\"The Massachusetts Institute of Technology_ORG\"]\n",
    "\n",
    "# all_edges = graph.edge(v1, v2, all_edges=True)\n",
    "# for i, edge in enumerate(all_edges):\n",
    "#   print(i, graph.edge_properties[\"sentence\"][edge])\n",
    "\n",
    "for alias in graph.vertex_properties[\"aliases\"][v2]:\n",
    "    print(alias)\n",
    "    print(\"-----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bool(graph.vertex_properties[\"terminal\"][v1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test saving graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.save(\"/content/drive/MyDrive/NLP_Project/Data/mit.gt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.all import load_graph\n",
    "\n",
    "graph2 = load_graph(\"/content/drive/MyDrive/NLP_Project/Data/mit.gt\")\n",
    "all_edges = graph2.edge(v1, v2, all_edges=True)\n",
    "for i, edge in enumerate(all_edges):\n",
    "    print(i, graph2.edge_properties[\"sentence\"][edge])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "def graph_to_string(graph):\n",
    "    # Use BytesIO for binary data\n",
    "    buffer = io.BytesIO()\n",
    "    graph.save(buffer)  # Save in default .gt format\n",
    "    return buffer.getvalue()  # Return the binary data\n",
    "\n",
    "\n",
    "test_dict = {\"data\": {\"more_data\": graph_to_string(graph)}}\n",
    "\n",
    "with open(\"/content/drive/MyDrive/NLP_Project/Data/dataset_with_graphs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_graph(graph_string):\n",
    "    # Use BytesIO to read the binary data\n",
    "    buffer = io.BytesIO(graph_string)\n",
    "    graph = gt.Graph()  # Create an empty graph\n",
    "    graph.load(buffer)  # Load the graph from the buffer\n",
    "    return graph\n",
    "\n",
    "\n",
    "with open(\"/content/drive/MyDrive/NLP_Project/Data/dataset_with_graphs.pkl\", \"rb\") as f:\n",
    "    test_dict_retrieved = pickle.load(f)\n",
    "\n",
    "graph_retrieved = string_to_graph(test_dict_retrieved[\"data\"][\"more_data\"])\n",
    "all_edges = graph_retrieved.edge(v1, v2, all_edges=True)\n",
    "# print(test_dict_retrieved[\"data\"])\n",
    "for i, edge in enumerate(all_edges):\n",
    "    print(i, graph_retrieved.edge_properties[\"sentence\"][edge])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphtool_to_networkx_multigraph(gt_graph):\n",
    "    \"\"\"\n",
    "    Converts a graph-tool undirected multigraph to a NetworkX MultiGraph.\n",
    "    Be careful as it gets rid of all edge properties. Should only be used for visualizing graph\n",
    "\n",
    "    Args:\n",
    "        gt_graph (graph_tool.Graph): The input graph-tool graph (undirected multigraph).\n",
    "\n",
    "    Returns:\n",
    "        nx_multigraph (networkx.MultiGraph): The equivalent NetworkX MultiGraph.\n",
    "    \"\"\"\n",
    "    # Create an empty NetworkX MultiGraph\n",
    "    nx_multigraph = nx.MultiGraph()\n",
    "\n",
    "    # Add nodes with properties\n",
    "    for v in gt_graph.vertices():\n",
    "        # Add node properties if they exist\n",
    "        node_properties = {k: vprop[v] for k, vprop in gt_graph.vp.items()}\n",
    "        nx_multigraph.add_node(int(v), **node_properties)\n",
    "\n",
    "    # Add edges with properties\n",
    "    edge_counter = {}\n",
    "    for e in gt_graph.edges():\n",
    "        # Determine a unique key for each edge between the same source and target\n",
    "        source = int(e.source())\n",
    "        target = int(e.target())\n",
    "        edge_key = edge_counter.get((source, target), 0)\n",
    "        edge_counter[(source, target)] = edge_key + 1\n",
    "\n",
    "        # Add edge properties if they exist\n",
    "        # edge_properties = {k: eprop[e] for k, eprop in gt_graph.ep.items()}\n",
    "        nx_multigraph.add_edge(source, target, key=edge_key)\n",
    "\n",
    "    return nx_multigraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "def visualize_graph(G):\n",
    "    net = Network(notebook=True)\n",
    "    # Load into PyVis\n",
    "\n",
    "    # Convert to NetworkX\n",
    "    nx_graph = graphtool_to_networkx_multigraph(G)  # if graph-tool\n",
    "    # nx_graph = graph #if networkx\n",
    "\n",
    "    # nx_graph = graph.to_networkx()\n",
    "    net.from_nx(nx_graph)\n",
    "    net.set_options(\"\"\"\n",
    "        var options = {\n",
    "            \"physics\": {\n",
    "            \"enabled\": false\n",
    "            }\n",
    "        }\n",
    "    \"\"\")\n",
    "\n",
    "    net.show(\"graph.html\")\n",
    "    display(HTML(\"graph.html\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_graph(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset Analysis Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diameter Aproximater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.all import Graph, GraphView, label_largest_component, pseudo_diameter\n",
    "\n",
    "\n",
    "def approximate_diameter(graph):\n",
    "    # Identify the largest connected component\n",
    "    largest_component = label_largest_component(graph)\n",
    "\n",
    "    # Extract the subgraph of the largest connected component\n",
    "    largest_cc_subgraph = GraphView(graph, vfilt=largest_component)\n",
    "\n",
    "    # Use pseudo_diameter to approximate the diameter\n",
    "    approx_diameter, _ = pseudo_diameter(largest_cc_subgraph)\n",
    "\n",
    "    return approx_diameter\n",
    "\n",
    "\n",
    "# print(\"Diameter: \", approximate_diameter(graph))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### % of parallel edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Terminal edges:\")\n",
    "for e in graph.edges():\n",
    "    if graph.edge_properties[\"terminal\"][e]:  # Check if the edge is terminal\n",
    "        print(\n",
    "            f\"Source: {graph.vertex_properties['label'][e.source()]} :label {graph.vertex_properties['ner_label'][e.source()]} , Target: label: {graph.vertex_properties['label'][e.target()]} id: {e.target()}, Sentence: {graph.edge_properties['sentence'][e]}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from graph_tool.all import Graph\n",
    "\n",
    "\n",
    "# Add example edges (if needed)\n",
    "# g.add_edge(source, target)\n",
    "def count_parallel_edges(g):\n",
    "    # Step 1: Create a list of all (source, target) pairs\n",
    "    # For undirected graphs, sort the source and target to ensure consistency\n",
    "    non_terminal_edges = [\n",
    "        (min(e.source(), e.target()), max(e.source(), e.target()))\n",
    "        for e in g.edges()\n",
    "        if not g.edge_properties[\"terminal\"][e]\n",
    "    ]\n",
    "\n",
    "    # Step 2: Count occurrences of each edge\n",
    "    edge_counts = Counter(non_terminal_edges)\n",
    "\n",
    "    # Step 3: Calculate the number of parallel edges\n",
    "    parallel_edge_count = sum(count for count in edge_counts.values() if count > 1)\n",
    "\n",
    "    # Step 4: Calculate the total number of edges\n",
    "    total_edge_count = g.num_edges()\n",
    "\n",
    "    # Step 5: Calculate the percentage of parallel edges\n",
    "    percentage_parallel_edges = (\n",
    "        (parallel_edge_count / total_edge_count) * 100 if total_edge_count > 0 else 0\n",
    "    )\n",
    "\n",
    "    return percentage_parallel_edges\n",
    "\n",
    "\n",
    "# print(f\"Percentage of parallel edges: {count_parallel_edges(graph):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Squad Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_json_schema(data, indent=0):\n",
    "    \"\"\"Recursively print the schema of a JSON object.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        print(\" \" * indent + \"{\")\n",
    "        for key, value in data.items():\n",
    "            print(\" \" * (indent + 2) + f'\"{key}\": {type(value).__name__}', end=\"\")\n",
    "            if isinstance(value, (dict, list)):\n",
    "                print(\" ->\")\n",
    "                print_json_schema(value, indent + 4)\n",
    "            else:\n",
    "                print(\",\")\n",
    "        print(\" \" * indent + \"}\")\n",
    "    elif isinstance(data, list):\n",
    "        print(\" \" * indent + \"[\")\n",
    "        if data:\n",
    "            print_json_schema(data[0], indent + 2)\n",
    "        else:\n",
    "            print(\" \" * (indent + 2) + \"Empty list\")\n",
    "        print(\" \" * indent + \"]\")\n",
    "    else:\n",
    "        print(\" \" * indent + f\"{type(data).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sent_graph_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
